{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Chapter 7 - Aggregations\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b3730b73c216:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Chapter 7 - Aggregations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f71500c0a90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"online-retail-dataset.csv\")\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **COUNT**\n",
    "La primera función que trataremos será **COUNT**, excepto que en este ejemplo funcionara como una transformación y no como una acción.\n",
    "* Especificar una columna para contar COUNT(“COLUMN_NAME”)\n",
    "* Todas las columnas usando COUNT(*) \n",
    "* COUNT(1) para contar cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"StockCode\")).show()\n",
    "#df.select(count(\"*\")).show()\n",
    "#df.select(count(col(\"StockCode\"))).show()\n",
    "#df.select(count(lit(1))).show()\n",
    "#spark.sql(\"SELECT COUNT(1) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **COUNTDISTINCT**\n",
    "A veces el numero total no es relevante, podria serlo la cantidad de grupos unicos. Esta funcion es mas relevante para columnas individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "#spark.sql(\"SELECT COUNT(DISTINCT *) FROM dftable\").show()\n",
    "#df.groupBy(\"StockCode\").agg(count(\"StockCode\").alias(\"count\")).filter(col(\"count\") > 1).show()\n",
    "#spark.sql(\"SELECT StockCode, count(StockCode) FROM dftable GROUP BY StockCode HAVING COUNT(StockCode) > 1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **APPROX_COUNT_DISTINCT**\n",
    "A menudo, nos encontramos trabajando con grandes conjuntos de datos y la funcion *COUNT DISTINCT* es irrelevante.  \n",
    "Hay momentos en que una aproximación con cierto grado de precisión funcionará bien, y\n",
    "para eso, puedes usar la función *APPROX_COUNT_DISTINCT*  \n",
    "\n",
    "Notarás que **approx_count_distinct** tomó otro parámetro con el que puedes especificar el error de estimación máximo permitido. En este caso, especificamos un error bastante grande y por lo tanto, recibe una respuesta que está bastante lejos pero se completa más rápido que countDistinct.  \n",
    "\n",
    "\n",
    "Verá ganancias de rendimiento mucho mayores con conjuntos de datos más grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "spark.sql(\"SELECT approx_count_distinct(StockCode, 0.1) FROM DFTABLE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **FIRST AND LAST**\n",
    "\n",
    "Puede obtener el primer y el último valor de una columna de un DataFrame haciendo uso de las funciones **FIRSTS** y **LAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()\n",
    "#spark.sql(\"SELECT first(StockCode), last(StockCode) FROM dfTable\").show()\n",
    "#df.orderBy(desc(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MIN AND MAX**\n",
    "\n",
    "Para extraer el minimo y maximo valor de un Dataframe podemos hacer uso de las funciones **MIN** y **MAX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n",
    "#spark.sql(\"SELECT min(Quantity), max(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SUM**\n",
    "Otra tarea simple es sumar todos los valores en una columna haciendo uso de la funcion **SUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum(\"Quantity\")).show()\n",
    "#spark.sql(\"SELECT sum(Quantity) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SUMDISTINCT**\n",
    "Además de sumar un total, también puede sumar un conjunto de valores unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/functions.py:214: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.select(sumDistinct(\"Quantity\")).show()\n",
    "#spark.sql(\"SELECT SUM(DISTINCT(Quantity)) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AVG**\n",
    "Aunque puede calcular el promedio dividiendo la suma por el conteo, Spark proporciona una forma más fácil de obtener ese valor a través de las funciones **AVG** o **MEAN**. En este ejemplo, usamos alias para reutilizar más fácilmente estas columnas más adelante.  \n",
    "\n",
    "También puede promediar todos los valores distintos especificando **distinct**. De hecho, la mayoría de las funciones agregadas\n",
    "admite hacerlo solo en valores distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases/total_transactions\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Aggregating to Complex Types**\n",
    "En Spark, puede realizar agregaciones no solo de valores numéricos usando fórmulas, también puede realizarlas en tipos complejos. Por ejemplo, podemos recopilar una lista de valores presentes en una columna determinada o solo los valores únicos.  \n",
    "\n",
    "Puede usar esto para realizar acceso programático en la canalización o pasar la colección completa en una función definida por el usuario (UDF).  \n",
    "*collect_list* - Genera un orden en la Matriz pero no elimina los datos duplicados  \n",
    "*collect_set* - No puiede guardar un orden existente pero si elimina los elementos duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "#df.groupBy(\"InvoiceNo\").agg(collect_list(\"Country\")).show()\n",
    "#df.groupBy(\"InvoiceNo\").agg(array_distinct(collect_list(\"Country\"))).show()\n",
    "#spark.sql(\"SELECT collect_set(Country), collect_set(Country) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grouping**\n",
    "\n",
    "Hasta ahora, solo hemos realizado agregaciones a nivel de DataFrame. Una tarea más común es realizar cálculos basados en grupos de datos. Esto generalmente se hace en datos categóricos para los cuales agrupamos nuestros datos en una columna y realizamos algunos cálculos en las otras columnas de ese grupo.  \n",
    "\n",
    "El primero será un conteo, tal como lo hicimos antes. Agruparemos por cada número de factura (InvoiceNo) y obtendremos el recuento de artículos en esa factura. Tenga en cuenta que esto devuelve otro DataFrame.  \n",
    "\n",
    "Esta agrupación la hacemos en dos fases. Primero especificamos la(s) columna(s) en las que nos gustaría agrupar, y luego especificamos la(s) agregación(es).  \n",
    "\n",
    "El primer paso devuelve un RelationalGroupedDataset y el segundo paso devuelve un DataFrame.  \n",
    "\n",
    "Como se mencionó, podemos especificar cualquier número de columnas en las que queremos agrupar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()\n",
    "#spark.sql(\"SELECT InvoiceNo, CustomerId, count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Grouping with Expressions**\n",
    "\n",
    "Esto le permite pasar expresiones arbitrarias que solo necesitan tener alguna agregación especificada. Incluso puede hacer cosas como alias de una columna después de transformarla para su uso posterior en su flujo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-------------------------+\n",
      "|InvoiceNo|Count_Quantity|Expression_Count_Quantity|\n",
      "+---------+--------------+-------------------------+\n",
      "|   536596|             6|                        6|\n",
      "|   536938|            14|                       14|\n",
      "|   537252|             1|                        1|\n",
      "|   537691|            20|                       20|\n",
      "|   538041|             1|                        1|\n",
      "|   538184|            26|                       26|\n",
      "|   538517|            53|                       53|\n",
      "|   538879|            19|                       19|\n",
      "|   539275|             6|                        6|\n",
      "|   539630|            12|                       12|\n",
      "|   540499|            24|                       24|\n",
      "|   540540|            22|                       22|\n",
      "|  C540850|             1|                        1|\n",
      "|   540976|            48|                       48|\n",
      "|   541432|             4|                        4|\n",
      "|   541518|           101|                      101|\n",
      "|   541783|            35|                       35|\n",
      "|   542026|             9|                        9|\n",
      "|   542375|             6|                        6|\n",
      "|  C542604|             8|                        8|\n",
      "+---------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "count(\"Quantity\").alias(\"Count_Quantity\"),\n",
    "expr(\"count(Quantity)\").alias(\"Expression_Count_Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Grouping with Maps**\n",
    "A veces, puede ser más fácil especificar sus transformaciones como una serie de mapas para los cuales la clave es la columna y el valor es la función de agregación (como una cadena). También puede reutilizar varios nombres de columna si los especifica en línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show()\n",
    "#spark.sql(\"SELECT InvoiceNo, avg(Quantity), stddev_pop(Quantity) FROM dfTable GROUP BY InvoiceNo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PIVOT**\n",
    "PySpark SQL proporciona la función para rotar los datos de una columna en varias columnas. Es una agregación donde uno de los valores de las columnas de agrupación se transpone en columnas individuales con datos distintos. En el siguiente ejemplo para obtener la cantidad total exportada a cada país de cada producto se puede hacer de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "|Banana |3000  |Canada |\n",
      "|Carrots|3000  |Canada |\n",
      "|Beans  |3000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\"),\\\n",
    "      (\"Banana\",3000,\"Canada\"),(\"Carrots\",3000,\"Canada\"),(\"Beans\",3000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df_pivot = spark.createDataFrame(data = data, schema = columns)\n",
    "df_pivot.printSchema()\n",
    "df_pivot.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |5000  |1600|\n",
      "|Banana |5000  |400  |null  |1000|\n",
      "|Carrots|5000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df_pivot.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Window Functions**\n",
    "También puede usar funciones de ventana para realizar algunas agregaciones únicas, ya sea calculando alguna agregación en una \"ventana\" específica de datos, que define usando una referencia a la\n",
    "datos actuales. Esta especificación de ventana determina qué filas se pasarán a esta función.\n",
    "\n",
    "Ahora bien, esto es un poco abstracto y probablemente similar a un grupo estándar, así que diferenciémoslos un poco más.\n",
    "\n",
    "Un groupby toma datos, y cada fila puede ir solo a una agrupación. Una función de ventana calcula un valor de retorno para cada fila de entrada de una tabla en función de un grupo de filas, denominado Frame.\n",
    "\n",
    "Cada fila puede caer en uno o más Frames. Un caso de uso común es echar un vistazo a un promedio móvil de algún valor para el cual cada fila representa un día. Si hiciera esto, cada fila terminaría en siete marcos diferentes. Cubriremos la definición de Frame un poco más adelante, pero para su referencia, Spark admite tres tipos de funciones de ventana: funciones de clasificación, funciones analíticas y funciones agregadas.  \n",
    "\n",
    "Para demostrarlo, agregaremos una columna de fecha que convertirá la fecha de su factura en una columna que contiene solo información de fecha (No tendra informacion de la hora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df_window_function = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df_window_function.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "\n",
    "dfWithDate.select(\"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso para una Window Function es crear una especificación de ventana. Es solo un concepto similar que describe cómo dividiremos nuestro grupo. El orden determina el orden dentro de una partición determinada y, por último, la especificación del Frame (la instrucción rowsBetween) establece qué filas se incluirán en el marco en función de su referencia a la fila de entrada actual. En el siguiente ejemplo, observamos todas las filas anteriores hasta la fila actual.  \n",
    "\n",
    "**Window.unboundedPreceding** denota la primera fila de la partición y **Window.unboundedFollowing** denota la última fila de la partición.\n",
    "\n",
    "**Window.currentRow** para especificar un valor actual en una fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos usar una función de agregación para obtener más información sobre cada cliente específico. Un ejemplo podría ser establecer la cantidad máxima de compra en todo momento. Para responder a esto, usamos las mismas funciones de agregación que vimos anteriormente al pasar un nombre de columna o una expresión.\n",
    "\n",
    "Además, indicamos la especificación de la ventana que define a qué marcos de datos se aplicará esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|max_quantity|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+------------+\n",
      "|   542504|    37413|                null|    5568| 1/28/2011 12:03|      0.0|      null|United Kingdom|        5568|\n",
      "|   556231|   85123A|                   ?|    4000|  6/9/2011 15:04|      0.0|      null|United Kingdom|        5568|\n",
      "|   560040|    23343| came coded as 20713|    3100| 7/14/2011 14:28|      0.0|      null|United Kingdom|        5568|\n",
      "|   546139|    84988|                   ?|    3000|  3/9/2011 16:35|      0.0|      null|United Kingdom|        5568|\n",
      "|   542505|   79063D|                null|    2560| 1/28/2011 12:04|      0.0|      null|United Kingdom|        5568|\n",
      "|   574941|    22197|      POPCORN HOLDER|    1820| 11/7/2011 17:42|     1.95|      null|United Kingdom|        5568|\n",
      "|   550460|   47556B|did  a credit  an...|    1300| 4/18/2011 13:18|      0.0|      null|United Kingdom|        5568|\n",
      "|   554550|   47566B|incorrectly credi...|    1300|  5/25/2011 9:57|      0.0|      null|United Kingdom|        5568|\n",
      "|   543258|   84611B|                null|    1287|  2/4/2011 16:06|      0.0|      null|United Kingdom|        5568|\n",
      "|   576365|    22197|      POPCORN HOLDER|    1130|11/14/2011 17:55|     1.95|      null|United Kingdom|        5568|\n",
      "|   540699|     POST|                null|    1000|  1/11/2011 9:32|      0.0|      null|United Kingdom|        5568|\n",
      "|   547966|      DOT|                null|    1000| 3/28/2011 15:49|      0.0|      null|United Kingdom|        5568|\n",
      "|   573114|    20713| wrongly coded 23343|    1000|10/27/2011 15:36|      0.0|      null|United Kingdom|        5568|\n",
      "|   543051|   79062D|                null|     960|  2/3/2011 10:15|      0.0|      null|United Kingdom|        5568|\n",
      "|   554857|     POST|                null|     800| 5/27/2011 10:08|      0.0|      null|United Kingdom|        5568|\n",
      "|   569830|    23343| wrongly coded 20713|     800| 10/6/2011 12:38|      0.0|      null|United Kingdom|        5568|\n",
      "|   575505|     POST|                null|     800|11/10/2011 10:29|      0.0|      null|United Kingdom|        5568|\n",
      "|   539494|    21479|                   ?|     752|12/20/2010 10:36|      0.0|      null|United Kingdom|        5568|\n",
      "|   565556|     POST|                null|     750|  9/5/2011 12:14|      0.0|      null|United Kingdom|        5568|\n",
      "|   576365|    22086|PAPER CHAIN KIT 5...|     688|11/14/2011 17:55|     6.95|      null|United Kingdom|        5568|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_between_df = df.withColumn(\"max_quantity\", max(\"Quantity\").over(windowSpec))\n",
    "\n",
    "range_between_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notará que esto devuelve una columna (o expresiones). Ahora podemos usar esto en una declaración de selección (SELECT) de un DataFrame. Sin embargo, antes de hacerlo, crearemos el rango de cantidad de compra. Para hacer eso, usamos la función dense_rank para determinar qué fecha tuvo la cantidad máxima de compra para cada cliente. Usamos dense_rank en lugar de rank para evitar brechas en la secuencia de clasificación cuando hay valores empatados (o en nuestro caso, filas duplicadas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto también devuelve una columna que podemos usar en declaraciones de selección. Ahora podemos realizar una selección para ver los valores de ventana calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT CustomerId, date, Quantity,\\\n",
    "rank(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "ORDER BY Quantity DESC NULLS LAST\\\n",
    "ROWS BETWEEN\\\n",
    "UNBOUNDED PRECEDING AND\\\n",
    "CURRENT ROW) as rank,\\\n",
    "dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "ORDER BY Quantity DESC NULLS LAST\\\n",
    "ROWS BETWEEN\\\n",
    "UNBOUNDED PRECEDING AND\\\n",
    "CURRENT ROW) as dRank,\\\n",
    "max(Quantity) OVER (PARTITION BY CustomerId, date\\\n",
    "ORDER BY Quantity DESC NULLS LAST\\\n",
    "ROWS BETWEEN\\\n",
    "UNBOUNDED PRECEDING AND\\\n",
    "CURRENT ROW) as maxPurchase\\\n",
    "FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
