{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9de4d6",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto y extracción de características en PySpark (para el análisis de sentimiento)\n",
    "\n",
    "En el presente notebook, procesaremos un conjunto de datos compuestos por tweets con el propósito de realizar un análisis de sentimiento con el módulo de regresión logística en Apache Spark. Se utilizará Spark ML y Spark NLP para las etapas de transformación de los datos e entrenamiento y evaluación del modelo de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8551211",
   "metadata": {},
   "source": [
    "## 1. Importar las librerías de Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d05a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark') #Especificar la ruta de Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413a388",
   "metadata": {},
   "source": [
    "## 2. Importar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7d9cb",
   "metadata": {},
   "source": [
    "Importaremos datos del Twitter en csv conteniendo tweets en la columna `select_text` y clases en la columna `sentiment` que representan el sentimiento de cada tweet (\"negativo\", \"neutral\" o \"positivo\").\n",
    "\n",
    "Dicho conjunto de datos fue utilizado en la competición **Tweet Sentiment Extraction** de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f06d52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iniciar SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Leer los datos\n",
    "df = spark\\\n",
    "     .read\\\n",
    "     .option(\"header\", \"true\")\\\n",
    "     .option(\"inferSchema\", True)\\\n",
    "     .csv(\"train.csv\")\\ # Especificar la ruta del archivo train.csv\n",
    "     .select(\"selected_text\", \"sentiment\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Reducir el número de Shuffle partitions para 5\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980a55b",
   "metadata": {},
   "source": [
    "## 3. Realizar una limpieza en los datos\n",
    "\n",
    "Antes de preprocesarlos, es necesario que nuestros datos estén limpios. Es imperativo explorar los datos para encontrar errores e imperfecciones que pueden tener un impacto negativo durante la etapa de preprocesamiento.\n",
    "\n",
    "Es muy importante asegurarnos de que **no hayan valores faltantes o nulos en el conjunto de datos**. La presencia de estos valores es la causa frecuente de problemas cuando trabajamos con Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar los valores faltantes o nulos\n",
    "df_cleaned = df\\\n",
    "            .dropna()\\\n",
    "            .select(\"selected_text\",\"sentiment\")\n",
    "\n",
    "# Contar la cantidad de filas del conjunto de datos\n",
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4d6ef",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de los datos con RegexTokenizer y StopWordsRemover\n",
    "\n",
    "Utilizaremos los siguientes **tranformers** para preprocesar nuestros datos:\n",
    "1. RegexTokenizer: transforma el texto en un conjunto de tokens (palabras) aplicando una regular expression (regex); y\n",
    "2. StopWordsRemover: remueve los tokens frecuentes de cada texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer las palabras de cada texto mediante una expresión regular (regex)\n",
    "regextokenizer = RegexTokenizer(inputCol=\"selected_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Remover las palabras comúnes del texto\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(englishStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"preprocessed\")\n",
    "\n",
    "# Construir la pipeline de preprocesamiento\n",
    "pipeline = Pipeline(stages=[regextokenizer, stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la pipeline de preprocesamiento\n",
    "pipelineFit = pipeline.fit(df_cleaned)\n",
    "countvectorizer_transformed = pipelineFit.transform(df_cleaned)\n",
    "\n",
    "# Remover filas con arrays vacios\n",
    "filtered = countvectorizer_transformed.filter(f.size('preprocessed') > 0)\n",
    "\n",
    "# Seleccionar la variable de entrada y la variable de salida\n",
    "preprocessed = filtered.select(\"sentiment\", \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a912805",
   "metadata": {},
   "source": [
    "## 5. Extracción de características con el CountVectorizer\n",
    "\n",
    "En un modelo de **TF-IDF**, el CountVectorizer puede ser utilizado para calcular el TF o *Term Frequency*. El TF es un vector que representaría la ocurrencia de cada palabra dentro de cada documento. El IDF intentar asignar pesos a cada elemento del TF. Las palabras que aparecen con mayor frecuencia en los documentos reciben un peso menor en comparación con la palabras menos comúnes en los documentos.\n",
    "\n",
    "El CountVectorizer es un transformer que hace un recuento de cada palabra en el documento y los expresa en un vector escaso.\n",
    "\n",
    "También hemos transformado la variable de salida a una representación numérica de las categorías mediante el StringIndexer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926102a0",
   "metadata": {},
   "source": [
    "#### 5.1. Construir la pipeline de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar la variable de entrada en vectores de representación mediante el CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "    .setInputCol(\"preprocessed\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setVocabSize(500)\\\n",
    "    .setMinTF(1)\\\n",
    "    .setMinDF(2)\n",
    "\n",
    "# Aplicar el IDF\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"features\")\\\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "# Representar la variable de salida en términos numéricos\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "# Construir la pipeline\n",
    "pipeline = Pipeline(stages=[cv, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462a514",
   "metadata": {},
   "source": [
    "#### 5.2 Aplicar la pipeline de transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b2c25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(preprocessed)\n",
    "countvectorizer_transformed = pipelineFit.transform(preprocessed).select(\"features\", \"label\")\n",
    "countvectorizer_transformed.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee83620",
   "metadata": {},
   "source": [
    "#### 5.3 Crear un modelo de regresión logística y evaluarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer el fit y transform\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(countvectorizer_transformed)\n",
    "predictions = lrModel.transform(countvectorizer_transformed)\n",
    "\n",
    "# Hacer el predict\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"f1\")\n",
    "print(\"La precisión del modelo es del {:0.2f}%\".format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9243f83",
   "metadata": {},
   "source": [
    "## 6. Extracción de características con el HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796c6fa",
   "metadata": {},
   "source": [
    "#### 6.1. Construir la pipeline de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar la variable de entrada en vectores de representación mediante el HashingTF\n",
    "tf = HashingTF()\\\n",
    "    .setInputCol(\"preprocessed\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setNumFeatures(10000)\n",
    "\n",
    "# Aplicar el IDF\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"features\")\\\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "# Representar la variable de salida en términos numéricos\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "# Construir la pipeline\n",
    "pipeline = Pipeline(stages=[tf, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c62453",
   "metadata": {},
   "source": [
    "#### 6.2 Aplicar la pipeline de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(preprocessed)\n",
    "hashingtf_transformed = pipelineFit.transform(preprocessed).select(\"features\", \"label\")\n",
    "hashingtf_transformed.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3080df",
   "metadata": {},
   "source": [
    "#### 6.3 Crear un modelo de regresión logística y evaluarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer el fit y transform\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(hashingtf_transformed)\n",
    "predictions = lrModel.transform(hashingtf_transformed)\n",
    "\n",
    "# Evaluar con MulticlassClassificationEvaluator\n",
    "print(\"La precisión del modelo es del {:0.2f}%\".format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838c4cb",
   "metadata": {},
   "source": [
    "## 7. Extracción de características con el Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ec0de",
   "metadata": {},
   "source": [
    "#### 7.1. Construir y aplicar la pipeline de extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0dc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"preprocessed\",\n",
    "outputCol=\"features\")\n",
    "\n",
    "# Representar la variable de salida en términos numéricos\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "# Aplicar la pipeline de extracción\n",
    "pipeline = Pipeline(stages=[word2Vec, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(preprocessed)\n",
    "word2vec_transformed = pipelineFit.transform(preprocessed).select(\"features\", \"label\")\n",
    "word2vec_transformed.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa6b89",
   "metadata": {},
   "source": [
    "#### 7.2 Crear un modelo de regresión logística y evaluarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer el fit y transform\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(word2vec_transformed)\n",
    "predictions = lrModel.transform(word2vec_transformed)\n",
    "\n",
    "# Evaluar con MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"f1\")\n",
    "print(\"La precisión del modelo es del {:0.2f}%\".format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bab4ef",
   "metadata": {},
   "source": [
    "## 8 Consideraciones finales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d06b3",
   "metadata": {},
   "source": [
    "En este notebook, hemos realizado el preprocesamiento de texto, extracción de características y entrenado un modelo de regresión logística para clasificar el sentimiento de cada tweet, utilizando puramente los módulos de Spark Machine Learning. Con transformaciones sencillas, hemos lograr entrenar un modelo con una precisión del +80% sobre los datos de entrenamiento.\n",
    "\n",
    "Podríamos mejorar nuestras pipelines de procesamiento con Spark NLP, incorporando a nuestra pipeline otros procesadores como Stemmer y Lemmatizer para normalizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaac500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
